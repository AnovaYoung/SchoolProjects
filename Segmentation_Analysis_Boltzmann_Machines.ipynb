{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnovaYoung/SchoolProjects/blob/main/Segmentation_Analysis_Boltzmann_Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J3I_w34q3Yq"
      },
      "source": [
        "**A Brief Introduction to the Boltzmann Machine**\n",
        "\n",
        "A Boltzmann machine is a type of stochastic recurrent neural network that consists of a network of symmetrically connected units. It is used for learning and representing complex probability distributions.\n",
        "\n",
        "Boltzmann machines can be employed for tasks such as feature learning, dimensionality reduction, and classification, and they operate by minimizing the *energy* of the network through a process of *probabilistic sampling*.\n",
        "\n",
        "It consists of units, or nodes, that are either visible (input units) or hidden. Each node can be in one of two states: active or inactive.\n",
        "\n",
        "The nodes are connected with weighted edges, and the goal is to **minimize the energy of the network**, which is a function of the states of the nodes and the weights of the edges.\n",
        "\n",
        "\n",
        "Biswal, A., ... Hussain, Z. (n.d.). Boltzmann Machine. In Computer Science. ScienceDirect. Retrieved from https://www.sciencedirect.com/topics/computer-science/boltzmann-machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wPWr_szq3Yr",
        "outputId": "213f4eb9-46e1-43d3-8f51-fb22b608054f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
            "   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n",
            "   - -------------------------------------- 10.2/250.9 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 102.4/250.9 kB 1.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 250.9/250.9 kB 2.6 MB/s eta 0:00:00\n",
            "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
            "[notice] To update, run: C:\\Users\\manov\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnSFtzR9q3Ys",
        "outputId": "fff90b4f-953e-421b-bd78-0cdcfd89f65c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Invoice StockCode                          Description  Quantity  \\\n",
            "0  536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
            "1  536365     71053                  WHITE METAL LANTERN         6   \n",
            "2  536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
            "3  536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
            "4  536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
            "\n",
            "          InvoiceDate  Price  Customer ID         Country  \n",
            "0 2010-12-01 08:26:00   2.55      17850.0  United Kingdom  \n",
            "1 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
            "2 2010-12-01 08:26:00   2.75      17850.0  United Kingdom  \n",
            "3 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
            "4 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
            "  Invoice StockCode                          Description  Quantity  \\\n",
            "0  536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
            "1  536365     71053                  WHITE METAL LANTERN         6   \n",
            "2  536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
            "3  536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
            "4  536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
            "\n",
            "          InvoiceDate  Price  Customer ID         Country  \n",
            "0 2010-12-01 08:26:00   2.55      17850.0  United Kingdom  \n",
            "1 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
            "2 2010-12-01 08:26:00   2.75      17850.0  United Kingdom  \n",
            "3 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
            "4 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = r'\\Users\\manov\\Downloads\\M5-online+retail+ii\\online_retail_II.xlsx'\n",
        "data = pd.read_excel(file_path, sheet_name='Year 2010-2011')\n",
        "\n",
        "print(data.head())\n",
        "\n",
        "data_cleaned = data.dropna(subset=['Customer ID'])\n",
        "\n",
        "print(data_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kwH5jSoq3Ys"
      },
      "source": [
        "**Preprocessing Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEZ6CWSiq3Ys",
        "outputId": "251ea2ea-4767-45f1-c638-35aa0667d026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    0    1    2    3    4    5    6    7    8    9   ...   29   30   31   32  \\\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
            "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
            "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
            "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
            "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
            "\n",
            "    33   34   35   36        0         1   \n",
            "0  0.0  0.0  1.0  0.0 -0.024373 -0.013136  \n",
            "1  0.0  0.0  1.0  0.0 -0.024373 -0.001017  \n",
            "2  0.0  0.0  1.0  0.0 -0.016330 -0.010250  \n",
            "3  0.0  0.0  1.0  0.0 -0.024373 -0.001017  \n",
            "4  0.0  0.0  1.0  0.0 -0.024373 -0.001017  \n",
            "\n",
            "[5 rows x 39 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Encode categorical data\n",
        "encoder = OneHotEncoder()\n",
        "encoded_countries = encoder.fit_transform(data_cleaned[['Country']]).toarray()\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_cleaned[['Quantity', 'Price']])\n",
        "\n",
        "# Combine both\n",
        "preprocessed_data = pd.concat([pd.DataFrame(encoded_countries), pd.DataFrame(scaled_data)], axis=1)\n",
        "\n",
        "print(preprocessed_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTSEts2Fq3Ys"
      },
      "source": [
        "**Transform the Data**\n",
        "\n",
        "Now I'll create a binary matrix indicating whether each customer purchased an item (1) or not (0) over a given time period. This transformation will involve grouping data by customer ID and invoice date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbJCRviTq3Ys",
        "outputId": "7d22219e-a8e6-47e4-8808-496a3c1f5962"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\manov\\AppData\\Local\\Temp\\ipykernel_33564\\687147393.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  binary_data = data_cleaned.groupby(['Customer ID', 'InvoiceDate']).apply(lambda x: x['StockCode'].nunique()).unstack().fillna(0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "InvoiceDate  2010-12-01 08:26:00  2010-12-01 08:28:00  2010-12-01 08:34:00  \\\n",
            "Customer ID                                                                  \n",
            "12346.0                      0.0                  0.0                  0.0   \n",
            "12347.0                      0.0                  0.0                  0.0   \n",
            "12348.0                      0.0                  0.0                  0.0   \n",
            "12349.0                      0.0                  0.0                  0.0   \n",
            "12350.0                      0.0                  0.0                  0.0   \n",
            "\n",
            "InvoiceDate  2010-12-01 08:35:00  2010-12-01 08:45:00  2010-12-01 09:00:00  \\\n",
            "Customer ID                                                                  \n",
            "12346.0                      0.0                  0.0                  0.0   \n",
            "12347.0                      0.0                  0.0                  0.0   \n",
            "12348.0                      0.0                  0.0                  0.0   \n",
            "12349.0                      0.0                  0.0                  0.0   \n",
            "12350.0                      0.0                  0.0                  0.0   \n",
            "\n",
            "InvoiceDate  2010-12-01 09:01:00  2010-12-01 09:02:00  2010-12-01 09:09:00  \\\n",
            "Customer ID                                                                  \n",
            "12346.0                      0.0                  0.0                  0.0   \n",
            "12347.0                      0.0                  0.0                  0.0   \n",
            "12348.0                      0.0                  0.0                  0.0   \n",
            "12349.0                      0.0                  0.0                  0.0   \n",
            "12350.0                      0.0                  0.0                  0.0   \n",
            "\n",
            "InvoiceDate  2010-12-01 09:32:00  ...  2011-12-09 12:09:00  \\\n",
            "Customer ID                       ...                        \n",
            "12346.0                      0.0  ...                  0.0   \n",
            "12347.0                      0.0  ...                  0.0   \n",
            "12348.0                      0.0  ...                  0.0   \n",
            "12349.0                      0.0  ...                  0.0   \n",
            "12350.0                      0.0  ...                  0.0   \n",
            "\n",
            "InvoiceDate  2011-12-09 12:16:00  2011-12-09 12:19:00  2011-12-09 12:20:00  \\\n",
            "Customer ID                                                                  \n",
            "12346.0                      0.0                  0.0                  0.0   \n",
            "12347.0                      0.0                  0.0                  0.0   \n",
            "12348.0                      0.0                  0.0                  0.0   \n",
            "12349.0                      0.0                  0.0                  0.0   \n",
            "12350.0                      0.0                  0.0                  0.0   \n",
            "\n",
            "InvoiceDate  2011-12-09 12:21:00  2011-12-09 12:23:00  2011-12-09 12:25:00  \\\n",
            "Customer ID                                                                  \n",
            "12346.0                      0.0                  0.0                  0.0   \n",
            "12347.0                      0.0                  0.0                  0.0   \n",
            "12348.0                      0.0                  0.0                  0.0   \n",
            "12349.0                      0.0                  0.0                  0.0   \n",
            "12350.0                      0.0                  0.0                  0.0   \n",
            "\n",
            "InvoiceDate  2011-12-09 12:31:00  2011-12-09 12:49:00  2011-12-09 12:50:00  \n",
            "Customer ID                                                                 \n",
            "12346.0                      0.0                  0.0                  0.0  \n",
            "12347.0                      0.0                  0.0                  0.0  \n",
            "12348.0                      0.0                  0.0                  0.0  \n",
            "12349.0                      0.0                  0.0                  0.0  \n",
            "12350.0                      0.0                  0.0                  0.0  \n",
            "\n",
            "[5 rows x 20460 columns]\n"
          ]
        }
      ],
      "source": [
        "# Here I'm grouping data by Customer ID and InvoiceDate\n",
        "binary_data = data_cleaned.groupby(['Customer ID', 'InvoiceDate']).apply(lambda x: x['StockCode'].nunique()).unstack().fillna(0)\n",
        "\n",
        "# Convert purchase counts to binary values: 1 if purchased, 0 if not purchased\n",
        "binary_data[binary_data > 0] = 1\n",
        "\n",
        "# Display the binary data\n",
        "print(binary_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl_E04n2q3Ys"
      },
      "source": [
        "**Step 4 Train the Boltzmann Machine**\n",
        "\n",
        "Now I will train the Boltzmann machine using the training set with the goal of learning the underlying probability distribution of the data.\n",
        "I'm going to use the **BernoulliRBM** from the sklearn library to implement and train the Boltzmann machine. I'm going to code it in Tensorflow since my PyTorch is corrupted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOUE7Oo_q3Ys",
        "outputId": "cd03e6df-c1dc-477a-fa5a-3a59627b24da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\manov\\AppData\\Local\\Temp\\ipykernel_33564\\2039621979.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  binary_data = data_cleaned.groupby(['Customer ID', 'InvoiceDate']).apply(lambda x: x['StockCode'].nunique()).unstack().fillna(0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Reconstruction Error: 0.003938\n",
            "Epoch 2/10, Reconstruction Error: 0.003938\n",
            "Epoch 3/10, Reconstruction Error: 0.003937\n",
            "Epoch 4/10, Reconstruction Error: 0.003936\n",
            "Epoch 5/10, Reconstruction Error: 0.003936\n",
            "Epoch 6/10, Reconstruction Error: 0.003935\n",
            "Epoch 7/10, Reconstruction Error: 0.003934\n",
            "Epoch 8/10, Reconstruction Error: 0.003934\n",
            "Epoch 9/10, Reconstruction Error: 0.003933\n",
            "Epoch 10/10, Reconstruction Error: 0.003932\n",
            "Training set reconstruction error: 0.249993\n",
            "Test set reconstruction error: 0.250001\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "file_path = r'\\Users\\manov\\Downloads\\M5-online+retail+ii\\online_retail_II.xlsx'\n",
        "data = pd.read_excel(file_path, sheet_name='Year 2010-2011')\n",
        "\n",
        "# Cleaning the data by removing incomplete entries\n",
        "data_cleaned = data.dropna(subset=['Customer ID'])\n",
        "\n",
        "# Group data by Customer ID and InvoiceDate and then create a binary matrix for purchases\n",
        "binary_data = data_cleaned.groupby(['Customer ID', 'InvoiceDate']).apply(lambda x: x['StockCode'].nunique()).unstack().fillna(0)\n",
        "binary_data[binary_data > 0] = 1\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test = train_test_split(binary_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "\n",
        "class RBM:\n",
        "    def __init__(self, n_visible, n_hidden, learning_rate=0.01):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initializing the weights and biases\n",
        "        self.weights = tf.Variable(tf.random.normal([self.n_visible, self.n_hidden], stddev=0.01))\n",
        "        self.h_bias = tf.Variable(tf.zeros([self.n_hidden]))\n",
        "        self.v_bias = tf.Variable(tf.zeros([self.n_visible]))\n",
        "\n",
        "    def sample_prob(self, probs):\n",
        "        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\n",
        "\n",
        "    def forward_pass(self, v):\n",
        "        h_prob = tf.nn.sigmoid(tf.matmul(v, self.weights) + self.h_bias)\n",
        "        h_sample = self.sample_prob(h_prob)\n",
        "        return h_prob, h_sample\n",
        "\n",
        "    def backward_pass(self, h):\n",
        "        v_prob = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.weights)) + self.v_bias)\n",
        "        v_sample = self.sample_prob(v_prob)\n",
        "        return v_prob, v_sample\n",
        "\n",
        "    def train(self, X, batch_size=64, n_epochs=10):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(X).batch(batch_size)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            epoch_error = 0\n",
        "            for batch in dataset:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    v0 = batch\n",
        "                    h0_prob, h0_sample = self.forward_pass(v0)\n",
        "                    v1_prob, v1_sample = self.backward_pass(h0_sample)\n",
        "                    h1_prob, _ = self.forward_pass(v1_sample)\n",
        "\n",
        "                    positive_grad = tf.matmul(tf.transpose(v0), h0_prob)\n",
        "                    negative_grad = tf.matmul(tf.transpose(v1_sample), h1_prob)\n",
        "\n",
        "                    loss = tf.reduce_mean(tf.square(v0 - v1_prob))\n",
        "\n",
        "                gradients = tape.gradient(loss, [self.weights, self.h_bias, self.v_bias])\n",
        "                optimizer = tf.optimizers.SGD(self.learning_rate)\n",
        "                optimizer.apply_gradients(zip(gradients, [self.weights, self.h_bias, self.v_bias]))\n",
        "\n",
        "                epoch_error += loss.numpy()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{n_epochs}, Reconstruction Error: {epoch_error / len(X):.6f}')\n",
        "\n",
        "    def reconstruct(self, v):\n",
        "        h_prob, h_sample = self.forward_pass(v)\n",
        "        v_prob, v_sample = self.backward_pass(h_sample)\n",
        "        return v_prob\n",
        "\n",
        "# Npw initializing the RBM\n",
        "n_visible = X_train.shape[1]\n",
        "n_hidden = 100\n",
        "rbm = RBM(n_visible, n_hidden)\n",
        "\n",
        "# Training the model\n",
        "rbm.train(X_train, batch_size=64, n_epochs=10)\n",
        "\n",
        "# Function to compute reconstruction error\n",
        "def reconstruction_error(rbm, data):\n",
        "    v = tf.constant(data, dtype=tf.float32)\n",
        "    v_reconstructed = rbm.reconstruct(v)\n",
        "    error = tf.reduce_mean(tf.square(v - v_reconstructed))\n",
        "    return error.numpy()\n",
        "\n",
        "# reconstruction error for training and testing sets\n",
        "train_error = reconstruction_error(rbm, X_train)\n",
        "test_error = reconstruction_error(rbm, X_test)\n",
        "\n",
        "print(f\"Training set reconstruction error: {train_error:.6f}\")\n",
        "print(f\"Test set reconstruction error: {test_error:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcJ6J18wq3Ys"
      },
      "source": [
        "The training and test reconstruction errors have been successfully calculated and the RBM has been trained. However, there's a deprecation warning related to the use of DataFrameGroupBy.apply. I'll definetly ned to address this warning to ensure my code is clean and future-proof.\n",
        "\n",
        "To do so I need to modify the line where the binary data is created. I will explicitly exclude the grouping columns from the opperation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTrJKKdaq3Yt"
      },
      "outputs": [],
      "source": [
        "# First group the data by Customer ID and InvoiceDate and then create a binary matrix for purchases\n",
        "grouped = data_cleaned.groupby(['Customer ID', 'InvoiceDate'])['StockCode'].nunique()\n",
        "binary_data = grouped.unstack().fillna(0)\n",
        "binary_data[binary_data > 0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkL5GFQcq3Yt",
        "outputId": "6cb05525-1daa-412d-82fc-d8855bc2cd09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Reconstruction Error: -17.709074\n",
            "Epoch 2/10, Reconstruction Error: -43.134250\n",
            "Epoch 3/10, Reconstruction Error: -58.942324\n",
            "Epoch 4/10, Reconstruction Error: -68.630678\n",
            "Epoch 5/10, Reconstruction Error: -74.400723\n",
            "Epoch 6/10, Reconstruction Error: -77.651505\n",
            "Epoch 7/10, Reconstruction Error: -79.063146\n",
            "Epoch 8/10, Reconstruction Error: -79.106429\n",
            "Epoch 9/10, Reconstruction Error: -77.998898\n",
            "Epoch 10/10, Reconstruction Error: -75.989487\n",
            "Training set reconstruction error: 0.144916\n",
            "Test set reconstruction error: 0.144841\n"
          ]
        }
      ],
      "source": [
        "# Now rewrite the code with the updated prerpocessing steps.\n",
        "\n",
        "file_path = r'\\Users\\manov\\Downloads\\M5-online+retail+ii\\online_retail_II.xlsx'\n",
        "data = pd.read_excel(file_path, sheet_name='Year 2010-2011')\n",
        "\n",
        "data_cleaned = data.dropna(subset=['Customer ID'])\n",
        "\n",
        "grouped = data_cleaned.groupby(['Customer ID', 'InvoiceDate'])['StockCode'].nunique()\n",
        "binary_data = grouped.unstack().fillna(0)\n",
        "binary_data[binary_data > 0] = 1\n",
        "\n",
        "X_train, X_test = train_test_split(binary_data, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "\n",
        "class RBM:\n",
        "    def __init__(self, n_visible, n_hidden, learning_rate=0.01):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = tf.Variable(tf.random.normal([self.n_visible, self.n_hidden], stddev=0.01))\n",
        "        self.h_bias = tf.Variable(tf.zeros([self.n_hidden]))\n",
        "        self.v_bias = tf.Variable(tf.zeros([self.n_visible]))\n",
        "\n",
        "    def sample_prob(self, probs):\n",
        "        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\n",
        "\n",
        "    def forward_pass(self, v):\n",
        "        h_prob = tf.nn.sigmoid(tf.matmul(v, self.weights) + self.h_bias)\n",
        "        h_sample = self.sample_prob(h_prob)\n",
        "        return h_prob, h_sample\n",
        "\n",
        "    def backward_pass(self, h):\n",
        "        v_prob = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.weights)) + self.v_bias)\n",
        "        v_sample = self.sample_prob(v_prob)\n",
        "        return v_prob, v_sample\n",
        "\n",
        "    def forward(self, v):\n",
        "        p_h, h = self.forward_pass(v)\n",
        "        p_v, v = self.backward_pass(h)\n",
        "        return v\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        vbias_term = tf.reduce_sum(tf.matmul(v, tf.expand_dims(self.v_bias, 1)), axis=1)\n",
        "        wx_b = tf.matmul(v, self.weights) + self.h_bias\n",
        "        hidden_term = tf.reduce_sum(tf.math.log1p(tf.exp(wx_b)), axis=1)\n",
        "        return -vbias_term - hidden_term\n",
        "\n",
        "def train_rbm(rbm, data, lr=0.01, batch_size=64, n_epochs=10):\n",
        "    optimizer = tf.optimizers.SGD(lr)\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_error = 0.0\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i+batch_size]\n",
        "            v0 = batch\n",
        "            h0_prob, h0_sample = rbm.forward_pass(v0)\n",
        "            v1_prob, v1_sample = rbm.backward_pass(h0_sample)\n",
        "            h1_prob, _ = rbm.forward_pass(v1_sample)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                positive_grad = tf.matmul(tf.transpose(v0), h0_prob)\n",
        "                negative_grad = tf.matmul(tf.transpose(v1_sample), h1_prob)\n",
        "                loss = tf.reduce_mean(rbm.free_energy(v0) - rbm.free_energy(v1_sample))\n",
        "\n",
        "            gradients = tape.gradient(loss, [rbm.weights, rbm.h_bias, rbm.v_bias])\n",
        "            optimizer.apply_gradients(zip(gradients, [rbm.weights, rbm.h_bias, rbm.v_bias]))\n",
        "            epoch_error += loss.numpy()\n",
        "        print(f'Epoch {epoch+1}/{n_epochs}, Reconstruction Error: {epoch_error / len(data):.6f}')\n",
        "\n",
        "n_visible = X_train.shape[1]\n",
        "n_hidden = 100\n",
        "rbm = RBM(n_visible, n_hidden)\n",
        "\n",
        "train_rbm(rbm, X_train, lr=0.01, batch_size=64, n_epochs=10)\n",
        "\n",
        "def reconstruction_error(rbm, data):\n",
        "    v = tf.constant(data, dtype=tf.float32)\n",
        "    v_reconstructed = rbm.forward(v)\n",
        "    error = tf.reduce_mean(tf.square(v - v_reconstructed))\n",
        "    return error.numpy()\n",
        "train_error = reconstruction_error(rbm, X_train)\n",
        "test_error = reconstruction_error(rbm, X_test)\n",
        "\n",
        "print(f\"Training set reconstruction error: {train_error:.6f}\")\n",
        "print(f\"Test set reconstruction error: {test_error:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQLUUyvHq3Yt"
      },
      "source": [
        "The previous implementation's reconstruction error being almost constant showed that the model wasn't learning effectively. The high reconstruction error on both training and test sets further confirms that the model was not capturing the patterns in the data.\n",
        "\n",
        "In contrast, the updated implementation shows a significant decrease in the reconstruction error over epochs, this is pretty effective learning. The negative values of reconstruction error (free energy) suggest that the RBM is successfully minimizing the energy function,this is exactly what I wanted! The lower reconstruction errors for both the training and test sets indicate better performance in capturing the data patterns."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}